{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the data\n",
    "\n",
    "As a first, I read the original data set (in the folder ``input/raw``):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-07T15:29:30.526352Z",
     "start_time": "2020-09-07T15:29:27.116602Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(812132, 5)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"../input/raw/stackexchange_812k.csv\")\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-07T15:29:30.544053Z",
     "start_time": "2020-09-07T15:29:30.528115Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>comment_id</th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Eliciting priors from experts</td>\n",
       "      <td>title</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>What is normality?</td>\n",
       "      <td>title</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>What are some valuable Statistical Analysis op...</td>\n",
       "      <td>title</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Assessing the significance of differences in d...</td>\n",
       "      <td>title</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The Two Cultures: statistics vs. machine learn...</td>\n",
       "      <td>title</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   post_id  parent_id  comment_id  \\\n",
       "0        1        NaN         NaN   \n",
       "1        2        NaN         NaN   \n",
       "2        3        NaN         NaN   \n",
       "3        4        NaN         NaN   \n",
       "4        6        NaN         NaN   \n",
       "\n",
       "                                                text category  \n",
       "0                      Eliciting priors from experts    title  \n",
       "1                                 What is normality?    title  \n",
       "2  What are some valuable Statistical Analysis op...    title  \n",
       "3  Assessing the significance of differences in d...    title  \n",
       "4  The Two Cultures: statistics vs. machine learn...    title  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic cleaning\n",
    "\n",
    "I take the first text for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-07T15:29:30.556180Z",
     "start_time": "2020-09-07T15:29:30.545518Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Eliciting priors from experts'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.text[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting to lower case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-07T15:29:30.566707Z",
     "start_time": "2020-09-07T15:29:30.557664Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eliciting priors from experts'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.text[0].lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting all strings to lower:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-07T15:29:31.176668Z",
     "start_time": "2020-09-07T15:29:30.568287Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['eliciting priors from experts', 'what is normality?', 'what are some valuable statistical analysis open source projects?', 'assessing the significance of differences in distributions', 'the two cultures: statistics vs. machine learning?', 'locating freely available data samples', 'so how many staticians *does* it take to screw in a lightbulb?', 'under what conditions should likert scales be used as ordinal or interval data?', 'multivariate interpolation approaches', 'forecasting demographic census', 'bayesian and frequentist reasoning in plain english', 'finding the pdf given the cdf', 'tools for modeling financial time series', 'what is a standard deviation?', 'testing random variate generation algorithms', 'what is the meaning of p values and t values in statistical tests?', 'r packages for seasonality analysis', 'examples for teaching: correlation does not mean causation', 'pseudo-random number generation algorithms', 'explain data visualization', 'clustering of large, heavy-tailed dataset', 'pca on correlation or covariance?', 'why do us and uk schools teach different methods of calculating the standard deviation?', 'can someone please explain the back-propagation algorithm?', 'what r packages do you find most useful in your daily work?', 'where can i find useful r tutorials with various implementations?', 'robust nonparametric estimation of hazard/survival functions based on low count data', 'how large a difference can be expected between standard garch and asymmetric garch volatility forecasts?', 'what are good basic statistics to use for ordinal data?', 'what is your favorite data visualization blog?', \"power of holm's multiple comparison testing compared to others\", 'what are some good frameworks for method selection?', 'what statistical blogs would you recommend?', 'why square the difference instead of taking the absolute value in standard deviation?', 'what is the best introductory bayesian statistics textbook?', 'clojure versus r: advantages and disadvantages for data analysis', 'algorithms to compute the running median?', 'free resources for learning r', 'free dataset resources?', 'can one use multiple regression to predict one principal component (pc) from several other pcs?']\n"
     ]
    }
   ],
   "source": [
    "texts = [i.lower() for i in df.text]\n",
    "print(texts[0:40])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The assignment hint towards the presence of tags and mathematical equation that need to be cleaned. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-07T15:29:31.586012Z",
     "start_time": "2020-09-07T15:29:31.179059Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['time series for count data, with counts < 20', 'why is it possible to get significant f statistic (p<.001) but non-significant regressor t-tests?', 'symmetric fat-tailed distributions where $\\\\mathbb{e} e^x < \\\\infty$', 'formulate hypotheses when $\\\\mu_a < \\\\mu_b$ is different from $\\\\mu_a > \\\\mu_b$', 'winbugs error with zero values in binomial distribution: value of order of binomial <expr> must be greater than zero', 'mahalanobis distance via pca when $n<p$', 'do you reject the null hypothesis when $p < \\\\alpha$ or $p \\\\leq \\\\alpha$? ', 'small dimensional classification (< 20 features), one (or two) dominant predictors', 'sample size to tell if more than x% of the population can do <thing>', \"chi-squared vs fisher's exact test w/ 5x6 contingency table & some cells <5?\"]\n"
     ]
    }
   ],
   "source": [
    "tags = [i.lower() for i in df.text if \"<\" in i]\n",
    "print(tags[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-07T15:29:31.590512Z",
     "start_time": "2020-09-07T15:29:31.587588Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['time series for count data, with counts < 20', 'why is it possible to get significant f statistic (p<.001) but non-significant regressor t-tests?', 'symmetric fat-tailed distributions where ', 'formulate hypotheses when  is different from ', 'winbugs error with zero values in binomial distribution: value of order of binomial <expr> must be greater than zero', 'mahalanobis distance via pca when ', 'do you reject the null hypothesis when  or ? ', 'small dimensional classification (< 20 features), one (or two) dominant predictors', 'sample size to tell if more than x% of the population can do <thing>', \"chi-squared vs fisher's exact test w/ 5x6 contingency table & some cells <5?\"]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "print([re.sub('\\$(.*?)\\$', '', s) for s in tags[0:10]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-07T15:29:31.600331Z",
     "start_time": "2020-09-07T15:29:31.592056Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['time series for count data, with counts < 20', 'why is it possible to get significant f statistic (p<.001) but non-significant regressor t-tests?', 'symmetric fat-tailed distributions where ', 'formulate hypotheses when  is different from ', 'winbugs error with zero values in binomial distribution: value of order of binomial <expr> must be greater than zero', 'mahalanobis distance via pca when ', 'do you reject the null hypothesis when  or ? ', 'small dimensional classification (< 20 features), one (or two) dominant predictors', 'sample size to tell if more than x% of the population can do <thing>', \"chi-squared vs fisher's exact test w/ 5x6 contingency table & some cells <5?\"]\n"
     ]
    }
   ],
   "source": [
    "remove_equations = [re.sub('\\$.*?\\$', '', s) for s in tags[0:10]]\n",
    "print(remove_equations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-07T15:29:31.608669Z",
     "start_time": "2020-09-07T15:29:31.601519Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['time series for count data, with counts < 20', 'why is it possible to get significant f statistic (p<.001) but non-significant regressor t-tests?', 'symmetric fat-tailed distributions where ', 'formulate hypotheses when  is different from ', 'winbugs error with zero values in binomial distribution: value of order of binomial  must be greater than zero', 'mahalanobis distance via pca when ', 'do you reject the null hypothesis when  or ? ', 'small dimensional classification (< 20 features), one (or two) dominant predictors', 'sample size to tell if more than x% of the population can do ', \"chi-squared vs fisher's exact test w/ 5x6 contingency table & some cells <5?\"]\n"
     ]
    }
   ],
   "source": [
    "remove_tags = [re.sub('<.*?>', '', s) for s in remove_equations]\n",
    "print(remove_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I would remove also the percentage sign:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-07T15:29:31.620134Z",
     "start_time": "2020-09-07T15:29:31.610016Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['time series for count data, with counts < 20', 'why is it possible to get significant f statistic (p<.001) but non-significant regressor t-tests?', 'symmetric fat-tailed distributions where ', 'formulate hypotheses when  is different from ', 'winbugs error with zero values in binomial distribution: value of order of binomial  must be greater than zero', 'mahalanobis distance via pca when ', 'do you reject the null hypothesis when  or ? ', 'small dimensional classification (< 20 features), one (or two) dominant predictors', 'sample size to tell if more than x of the population can do ', \"chi-squared vs fisher's exact test w/ 5x6 contingency table & some cells <5?\"]\n"
     ]
    }
   ],
   "source": [
    "remove_percentages = [re.sub('%', '', s) for s in remove_tags]\n",
    "print(remove_percentages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, for the full dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-07T15:29:35.081791Z",
     "start_time": "2020-09-07T15:29:31.622067Z"
    }
   },
   "outputs": [],
   "source": [
    "texts = [i.lower() for i in df.text]\n",
    "remove_equations = [re.sub('\\$.*?\\$', '', s) for s in texts]\n",
    "remove_tags = [re.sub('<.*?>', '', s) for s in remove_equations]\n",
    "remove_percentages = [re.sub('%', '', s) for s in remove_tags]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating word frequency\n",
    "\n",
    "In order to find typos or formula survived to the filters, I joined in a unique string all the texts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-07T15:29:35.972618Z",
     "start_time": "2020-09-07T15:29:35.083455Z"
    }
   },
   "outputs": [],
   "source": [
    "all_texts = \" \".join(remove_percentages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-07T15:29:35.984382Z",
     "start_time": "2020-09-07T15:29:35.973982Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eliciting priors from experts what is normality? what are some valuable statistical analysis open source projects? assessing the significance of differences in distributions the two cultures: statistics vs. machine learning? locating freely available data samples so how many staticians *does* it take to screw in a lightbulb? under what conditions should likert scales be used as ordinal or interval data? multivariate interpolation approaches forecasting demographic census bayesian and frequentist reasoning in plain english finding the pdf given the cdf tools for modeling financial time series what is a standard deviation? testing random variate generation algorithms what is the meaning of p values and t values in statistical tests? r packages for seasonality analysis examples for teaching: correlation does not mean causation pseudo-random number generation algorithms explain data visualization clustering of large, heavy-tailed dataset pca on correlation or covariance? why do us and uk schools teach different methods of calculating the standard deviation? can someone please explain the back-propagation algorithm? what r packages do you find most useful in your daily work? where can i find useful r tutorials with various implementations? robust nonparametric estimation of hazard/survival functions based on low count data how large a difference can be expected between standard garch and asymmetric garch volatility forecasts? what are good basic statistics to use for ordinal data? what is your favorite data visualization blog? power of holm's multiple comparison testing compared to others what are some good frameworks for method selection? what statistical blogs would you recommend? why square the difference instead of taking the absolute value in standard deviation? what is the best introductory bayesian statistics textbook? clojure versus r: advantages and disadvantages for data analysis algorithms to compute the running median? free resources for learning r free datas\n"
     ]
    }
   ],
   "source": [
    "print(all_texts[0:2000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then tokenizing using ``nltk``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-07T15:35:20.781235Z",
     "start_time": "2020-09-07T15:29:35.985686Z"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "words_list = word_tokenize(all_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-05T15:37:50.594101Z",
     "start_time": "2020-09-05T15:37:50.546731Z"
    }
   },
   "source": [
    "Using ``Counter`` to count the word frequency:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-07T15:35:26.528755Z",
     "start_time": "2020-09-07T15:35:20.782426Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "c = Counter(words_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-07T15:35:28.359640Z",
     "start_time": "2020-09-07T15:35:26.530011Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('fxns', 1),\n",
       " ('crash-course', 1),\n",
       " ('montone', 1),\n",
       " ('regresser', 1),\n",
       " ('//stats.stackexchange.com/a/301933/28500', 1),\n",
       " ('erxcerpt', 1),\n",
       " ('build/get', 1),\n",
       " ('//stats.stackexchange.com/questions/279918', 1),\n",
       " ('vocalizes', 1),\n",
       " ('cadgas', 1),\n",
       " ('outnumbering', 1),\n",
       " ('multivariate20regression', 1),\n",
       " ('q=multiple20regression', 1),\n",
       " ('a-lambda', 1),\n",
       " ('pc1+pc3', 1),\n",
       " ('//stats.stackexchange.com/questions/4220/', 1),\n",
       " ('night-sky', 1),\n",
       " ('//stats.stackexchange.com/questions/199207/why-do-t-test-assuming-equal-population-variance-and-t-test-not-assuming-equal-v',\n",
       "  1),\n",
       " ('rian', 1)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " c.most_common()[:-20:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing links\n",
    "\n",
    "The links should be probably removed; for the moment, replacing all of them simply with 'http'. Thus the procedure should be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-07T15:35:32.461638Z",
     "start_time": "2020-09-07T15:35:28.361680Z"
    }
   },
   "outputs": [],
   "source": [
    "texts = [i.lower() for i in df.text]\n",
    "remove_equations = [re.sub('\\$.*?\\$', '', s) for s in texts]\n",
    "remove_tags = [re.sub('<.*?>', '', s) for s in remove_equations]\n",
    "remove_percentages = [re.sub('%', '', s) for s in remove_tags]\n",
    "remove_links = [re.sub(r\"http\\S+\", \"http\", s) for s in remove_percentages]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-07T15:41:43.032984Z",
     "start_time": "2020-09-07T15:35:32.463491Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('fxns', 1),\n",
       " ('crash-course', 1),\n",
       " ('montone', 1),\n",
       " ('regresser', 1),\n",
       " ('erxcerpt', 1),\n",
       " ('build/get', 1),\n",
       " ('vocalizes', 1),\n",
       " ('cadgas', 1),\n",
       " ('outnumbering', 1),\n",
       " ('a-lambda', 1),\n",
       " ('pc1+pc3', 1),\n",
       " ('night-sky', 1),\n",
       " ('rian', 1),\n",
       " ('detoxication', 1),\n",
       " ('sf36', 1),\n",
       " ('q3-5', 1),\n",
       " ('score=7', 1),\n",
       " ('score=8', 1),\n",
       " ('t2w', 1),\n",
       " ('t1w', 1),\n",
       " ('23+10+12=t1i+t1w+t2w', 1),\n",
       " ('self-limiting', 1),\n",
       " ('wait-listing', 1),\n",
       " ('risk/calving', 1),\n",
       " ('deviations/variance', 1),\n",
       " ('matox', 1),\n",
       " ('varianc', 1),\n",
       " ('loan-status', 1),\n",
       " ('co-applicant', 1),\n",
       " ('1-fitted.vales', 1),\n",
       " ('fitted.values/', 1),\n",
       " ('physicsforums.com', 1),\n",
       " ('discrete/continuous/neither', 1),\n",
       " ('exp.wald', 1),\n",
       " ('ncp=exp.wald', 1),\n",
       " ('q=qnorm', 1),\n",
       " ('itcannot', 1),\n",
       " ('li.ear', 1),\n",
       " ('fundemental', 1),\n",
       " ('just-burned-out', 1),\n",
       " ('ms_caalis', 1),\n",
       " ('eifenvalue', 1),\n",
       " ('dony', 1),\n",
       " ('up-weeks', 1),\n",
       " ('up-week', 1),\n",
       " (\"method='neuralnet\", 1),\n",
       " ('data=train_hypep', 1),\n",
       " ('turtles~twine+mesh+black+blue+green+red+orange+yellow+synthetic+braided+mono+multi+x1+x2+x3+x4+x5+x16',\n",
       "  1),\n",
       " ('layer3=1:5', 1)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_texts = \" \".join(remove_links)\n",
    "words_list = word_tokenize(all_texts)\n",
    "c = Counter(words_list)\n",
    "c.most_common()[:-50:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing formulas\n",
    "\n",
    "One important filter is to remove math formulas in R:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-07T15:41:43.040407Z",
     "start_time": "2020-09-07T15:41:43.035589Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "formula = 'turtles ~twine +mesh+black+blue+green+red+orange+yellow+synthetic+braided+mono+multi+x1+x2+x3+x4+x5+ x16'\n",
    "re.sub('[a-z]*[0-9]* *\\~([a-z]*[0-9]* *\\+)* *[a-z]*[0-9]*', '', formula)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing parameter setting\n",
    "\n",
    "Another relevant filter can be on setting parameters in a call, like in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-07T15:41:43.067407Z",
     "start_time": "2020-09-07T15:41:43.042645Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', '']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters = [\"method='neuralnet\", 'data=train_hypep']\n",
    "[re.sub('[a-z_]*=[\\'|\\\"]*[0-9a-z_]*', '', s) for s in parameters]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus the procedure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-07T15:42:39.079641Z",
     "start_time": "2020-09-07T15:41:43.070952Z"
    }
   },
   "outputs": [],
   "source": [
    "texts = [i.lower() for i in df.text]\n",
    "remove_equations = [re.sub('\\$.*?\\$', '', s) for s in texts]\n",
    "remove_tags = [re.sub('<.*?>', '', s) for s in remove_equations]\n",
    "remove_percentages = [re.sub('%', '', s) for s in remove_tags]\n",
    "remove_links = [re.sub(r\"http\\S+\", \"http\", s) for s in remove_percentages]\n",
    "remove_formulas = [re.sub('[a-z]*[0-9]* *\\~([a-z]*[0-9]* *\\+)* *[a-z]*[0-9]*', '', s) for s in remove_links]\n",
    "remove_parameters = [re.sub('[a-z_]*=[\\'|\\\"]*[0-9a-z_]*', '', s) for s in remove_formulas]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-09-07T15:29:27.139Z"
    }
   },
   "outputs": [],
   "source": [
    "all_texts = \" \".join(remove_parameters)\n",
    "words_list = word_tokenize(all_texts)\n",
    "c = Counter(words_list)\n",
    "c.most_common()[:-50:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-09-07T15:29:27.141Z"
    }
   },
   "outputs": [],
   "source": [
    "df[\"text_tokenized\"] = [\" \".join(word_tokenize(s)) for s in remove_parameters]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-09-07T15:29:27.142Z"
    }
   },
   "outputs": [],
   "source": [
    "df.to_csv(\"../input/preprocessed/tokenized.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
